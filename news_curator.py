#!/usr/bin/env python3
"""Daily RSS News Curator â€” LLM-powered curation using Claude CLI."""

import json
import logging
import os
import re
import sqlite3
import subprocess
import sys
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from email.utils import parsedate_to_datetime
from html import unescape
from pathlib import Path
from urllib.request import urlopen, Request
from urllib.error import URLError, HTTPError
from xml.etree import ElementTree as ET

BASE_DIR = Path(__file__).parent
CONFIG_PATH = BASE_DIR / "config.json"
DB_PATH = BASE_DIR / "seen.db"
LOG_PATH = BASE_DIR / "curator.log"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(LOG_PATH, encoding="utf-8"),
        logging.StreamHandler(sys.stdout),
    ],
)
log = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Data
# ---------------------------------------------------------------------------

@dataclass
class Article:
    title: str
    link: str
    description: str
    pub_date: str
    source: str
    categories: list[str] = field(default_factory=list)
    score: int = 0
    summary: str = ""
    tags: list[str] = field(default_factory=list)
    reason: str = ""


def strip_html(text: str) -> str:
    text = re.sub(r"<[^>]+>", "", text)
    text = unescape(text)
    return re.sub(r"\s+", " ", text).strip()


def parse_pub_date(date_str: str) -> datetime | None:
    """Parse RSS/Atom date strings into timezone-aware datetime."""
    if not date_str or not date_str.strip():
        return None
    date_str = date_str.strip()
    # RFC 822 (RSS 2.0): "Sat, 22 Feb 2026 10:30:00 +0000"
    try:
        return parsedate_to_datetime(date_str)
    except Exception:
        pass
    # ISO 8601 (Atom): "2026-02-22T10:30:00Z"
    for fmt in ("%Y-%m-%dT%H:%M:%SZ", "%Y-%m-%dT%H:%M:%S%z",
                 "%Y-%m-%dT%H:%M:%S.%fZ", "%Y-%m-%dT%H:%M:%S.%f%z", "%Y-%m-%d"):
        try:
            dt = datetime.strptime(date_str, fmt)
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt
        except ValueError:
            continue
    return None


# ---------------------------------------------------------------------------
# SQLite dedup DB
# ---------------------------------------------------------------------------

def init_db() -> sqlite3.Connection:
    conn = sqlite3.connect(DB_PATH)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS seen_articles (
            url   TEXT PRIMARY KEY,
            title TEXT,
            source TEXT,
            seen_at TEXT DEFAULT (datetime('now'))
        )
    """)
    conn.commit()
    return conn


def is_seen(conn: sqlite3.Connection, url: str) -> bool:
    return conn.execute(
        "SELECT 1 FROM seen_articles WHERE url = ?", (url,)
    ).fetchone() is not None


def mark_seen(conn: sqlite3.Connection, articles: list[Article]):
    conn.executemany(
        "INSERT OR IGNORE INTO seen_articles (url, title, source) VALUES (?, ?, ?)",
        [(a.link, a.title, a.source) for a in articles],
    )
    conn.commit()


def cleanup_db(conn: sqlite3.Connection, days: int = 30):
    cutoff = (datetime.now() - timedelta(days=days)).isoformat()
    n = conn.execute("DELETE FROM seen_articles WHERE seen_at < ?", (cutoff,)).rowcount
    conn.commit()
    if n:
        log.info("Cleaned up %d old DB entries", n)


# ---------------------------------------------------------------------------
# Feed fetching & parsing
# ---------------------------------------------------------------------------

def fetch_feed(feed_cfg: dict) -> str | None:
    url = feed_cfg["url"]
    headers = feed_cfg.get("headers", {})
    req = Request(url)
    for k, v in headers.items():
        req.add_header(k, v)
    try:
        with urlopen(req, timeout=15) as resp:
            return resp.read().decode("utf-8", errors="replace")
    except HTTPError as e:
        if e.code == 403:
            log.warning("Feed %s returned 403 â€” skipping", feed_cfg["name"])
        else:
            log.error("Failed to fetch %s: HTTP %d", feed_cfg["name"], e.code)
        return None
    except URLError as e:
        log.error("Failed to fetch %s: %s", feed_cfg["name"], e.reason)
        return None


def parse_feed(xml_text: str, source_name: str) -> list[Article]:
    articles = []
    try:
        root = ET.fromstring(xml_text)
    except ET.ParseError as e:
        log.error("XML parse error for %s: %s", source_name, e)
        return []

    # Detect Atom vs RSS 2.0
    tag = root.tag.lower()
    if "feed" in tag:
        # Atom format
        ns = {"a": "http://www.w3.org/2005/Atom"}
        for entry in root.findall("a:entry", ns):
            title = (entry.findtext("a:title", "", ns) or "").strip()
            link_el = entry.find("a:link[@rel='alternate']", ns)
            if link_el is None:
                link_el = entry.find("a:link", ns)
            link = link_el.get("href", "") if link_el is not None else ""
            desc = strip_html(entry.findtext("a:content", "", ns)
                              or entry.findtext("a:summary", "", ns) or "")
            pub_date = entry.findtext("a:published", "", ns) or entry.findtext("a:updated", "", ns) or ""
            categories = [c.get("term", "") for c in entry.findall("a:category", ns) if c.get("term")]

            if not title or not link:
                continue
            articles.append(Article(
                title=title, link=link, description=desc[:500],
                pub_date=pub_date, source=source_name, categories=categories,
            ))
    else:
        # RSS 2.0 format
        for item in root.iter("item"):
            title = item.findtext("title", "").strip()
            link = item.findtext("link", "").strip()
            desc = strip_html(item.findtext("description", ""))
            pub_date = item.findtext("pubDate", "")
            categories = [c.text for c in item.findall("category") if c.text]

            if not title or not link:
                continue
            articles.append(Article(
                title=title, link=link, description=desc[:500],
                pub_date=pub_date, source=source_name, categories=categories,
            ))
    return articles


# ---------------------------------------------------------------------------
# Claude CLI curation
# ---------------------------------------------------------------------------

def _build_prompt(articles: list[Article], config: dict) -> str:
    """Build a single prompt combining system instructions + article list."""
    cur = config["curator"]
    interests = "\n".join(f"- {i}" for i in cur["interests"])
    min_score = config["scoring"]["min_score"]
    max_articles = cur.get("max_articles", 20)

    # Numbered article list
    lines = []
    for i, a in enumerate(articles):
        cats = f" [{', '.join(a.categories)}]" if a.categories else ""
        date_str = ""
        if a.pub_date:
            dt = parse_pub_date(a.pub_date)
            date_str = f"\në°œí–‰ì¼: {dt.strftime('%Y-%m-%d')}" if dt else f"\në°œí–‰ì¼: {a.pub_date}"
        lines.append(
            f"[{i}] [{a.source}]{cats}\n"
            f"ì œëª©: {a.title}{date_str}\n"
            f"ìš”ì•½: {a.description[:300]}"
        )
    article_text = "\n\n".join(lines)

    return f"""ë‹¹ì‹ ì€ 10ë…„ ì°¨ ì‹œë‹ˆì–´ í•€í…Œí¬ ë°±ì—”ë“œ ì—”ì§€ë‹ˆì–´ì´ìž í…Œí¬ ë¦¬ë“œìž…ë‹ˆë‹¤.
ë‹¨ìˆœí•œ 'ì‹ ê¸°ìˆ  ì†Œê°œ'ë³´ë‹¤ëŠ” ì‹œìŠ¤í…œ ì•ˆì •ì„±, ëŒ€ìš©ëŸ‰ íŠ¸ëž˜í”½ ì²˜ë¦¬, ë°ì´í„° ë¬´ê²°ì„±(Data Integrity), ë³´ì•ˆì— ì´ˆì ì„ ë§žì¶° ê¸°ì‚¬ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

ì‚¬ìš©ìž í”„ë¡œí•„:
{cur["persona"]}

ê´€ì‹¬ ë¶„ì•¼:
{interests}

## ì±„ì  ë£¨ë¸Œë¦­ (Fintech Backend Specialized)

9-10ì  [í•„ë… - ì•„í‚¤í…ì²˜/Deep Dive]:
- ê¸ˆìœµ ë„ë©”ì¸(ì›ìž¥ ì„¤ê³„, íŠ¸ëžœìž­ì…˜, ì •í•©ì„±)ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì‹¬ì¸µ ê¸°ìˆ  ì•„í‹°í´.
- ëŒ€ê·œëª¨ íŠ¸ëž˜í”½ ë¶„ì‚° ì²˜ë¦¬, ìž¥ì•  ëŒ€ì‘(Failover), ë°ì´í„°ë² ì´ìŠ¤ ë½í‚¹/ê²©ë¦¬ ìˆ˜ì¤€ì— ëŒ€í•œ ì‹¤ë¬´ì  ê²½í—˜ ê³µìœ .
- í•€í…Œí¬ ë³´ì•ˆ ê·œì œ(ë§ë¶„ë¦¬, ê°œì¸ì •ë³´ ì•”í˜¸í™”)ì™€ ê´€ë ¨ëœ ê¸°ìˆ ì  í•´ë²•.

7-8ì  [ì¶”ì²œ - Best Practice]:
- Java/Kotlin/Spring Boot, DB(MySQL, Redis), Kafka ë“± ë°±ì—”ë“œ ì½”ì–´ ê¸°ìˆ ì˜ ì„±ëŠ¥ ìµœì í™” ì‚¬ë¡€.
- MSA ì „í™˜, CI/CD íŒŒì´í”„ë¼ì¸ ê°œì„  ë“± ìƒì‚°ì„± í–¥ìƒ ì‚¬ë¡€.
- AI/LLMì˜ ì‹¤ë¬´ ì ìš© ì‚¬ë¡€ (ê°œë°œ ìƒì‚°ì„±, ì½”ë“œ ë¦¬ë·° ìžë™í™” ë“±).

4-6ì  [ì°¸ê³  - ì¼ë°˜ ë™í–¥]:
- í´ë¼ìš°ë“œ ë²¤ë”(AWS/GCP)ì˜ ì‹ ê·œ ê¸°ëŠ¥ ì¤‘ ë°±ì—”ë“œì™€ ì—°ê´€ëœ ê²ƒ.
- ì¼ë°˜ì ì¸ ê°œë°œ ë°©ë²•ë¡ ì´ë‚˜ ì†Œí”„íŠ¸ ìŠ¤í‚¬.

1-3ì  [ì œì™¸]:
- í”„ë¡ íŠ¸ì—”ë“œ(React, CSS ë“±), ëª¨ë°”ì¼ UI/UX ì „ìš© ê¸°ì‚¬.
- ê¸°ìˆ ì  ë‚´ìš©ì´ ì—†ëŠ” ë‹¨ìˆœ ì œí’ˆ í™ë³´, íˆ¬ìž ìœ ì¹˜, ê²½ì˜ì§„ ì¸í„°ë·°.
- ì£¼ê°€ ë³€ë™, ì•”í˜¸í™”í ì‹œì„¸ ë“± 'íˆ¬ìž ì •ë³´'.
- ê¸°ìˆ ì  ê¹Šì´ ì—†ì´ ìš©ì–´ë§Œ ë‚˜ì—´í•œ ê¸°ì‚¬.

## ê·œì¹™

1. ê° ê¸°ì‚¬ë¥¼ ìœ„ ë£¨ë¸Œë¦­ì— ë”°ë¼ 1-10ì ìœ¼ë¡œ í‰ê°€
2. {min_score}ì  ì´ìƒì¸ ê¸°ì‚¬ë§Œ ì„ íƒ
3. ìµœëŒ€ {max_articles}ê°œê¹Œì§€ ì„ íƒ
4. ìš”ì•½: [ë¬¸ì œ ìƒí™©] â†’ [ê¸°ìˆ ì  í•´ê²°ì±…] â†’ [ê²°ê³¼/Key Takeaway] êµ¬ì¡°ë¡œ ê±´ì¡°í•˜ê²Œ 3ë¬¸ìž¥ ì´ë‚´ í•œêµ­ì–´ ìš”ì•½. "ìœ ìµí•œ ê¸°ì‚¬ìž…ë‹ˆë‹¤" ê°™ì€ ë¯¸ì‚¬ì—¬êµ¬ ê¸ˆì§€.
5. í•œêµ­ì–´ íƒœê·¸ 1-3ê°œ ë¶€ì—¬
6. reason: "ì™œ ë‚´ê°€ ì´ê±¸ ì½ì–´ì•¼ í•˜ëŠ”ê°€"ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ìž‘ì„±. í˜„ìž¬ ì—…ë¬´ì— ì–´ë–»ê²Œ ì ìš© ê°€ëŠ¥í•œì§€ í•œ ì¤„ë¡œ ì„œìˆ .

ë°˜ë“œì‹œ ì•„ëž˜ JSON ë°°ì—´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´):
[
  {{
    "index": 0,
    "score": 8,
    "summary": "[ë¬¸ì œ] ... â†’ [í•´ê²°] ... â†’ [ê²°ê³¼] ...",
    "tags": ["íƒœê·¸1", "íƒœê·¸2"],
    "reason": "í˜„ìž¬ ê²°ì œ ëª¨ë“ˆ ë¦¬íŒ©í† ë§ì— ì°¸ê³ í•  ìˆ˜ ìžˆëŠ” DB Deadlock í•´ê²° íŒ¨í„´ í¬í•¨"
  }}
]

{min_score}ì  ì´ìƒì¸ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ []ì„ ë°˜í™˜í•˜ì„¸ìš”.

---

ë‹¤ìŒ {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ íë ˆì´ì…˜í•´ì£¼ì„¸ìš”:

{article_text}"""


def curate_with_claude(articles: list[Article], config: dict) -> list[Article]:
    """Call Claude CLI to curate articles."""
    cur = config["curator"]
    prompt = _build_prompt(articles, config)
    model = cur.get("model")

    log.info("Calling Claude CLI (%s) with %d articles â€¦", model or "default", len(articles))

    try:
        cmd = ["claude", "-p", prompt, "--output-format", "text", "--max-turns", "4"]
        if model:
            cmd.extend(["--model", model])
        env = {k: v for k, v in os.environ.items() if k != "CLAUDECODE"}
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300, env=env)
    except subprocess.TimeoutExpired:
        log.error("Claude CLI timed out")
        return []
    except FileNotFoundError:
        log.error("'claude' CLI not found. Install: npm install -g @anthropic-ai/claude-code")
        return []
    except Exception as e:
        log.error("Claude CLI unexpected error: %s", e)
        return []

    if result.returncode != 0:
        log.error("Claude CLI error (exit %d): %s", result.returncode, result.stderr[:500])
        return []

    text = result.stdout.strip()
    if not text:
        log.error("Empty response from Claude CLI")
        return []

    # Strip markdown code fences if present
    if text.startswith("```"):
        text = re.sub(r"^```(?:json)?\s*", "", text)
        text = re.sub(r"\s*```$", "", text)

    # Try to extract JSON array from response
    try:
        selections = json.loads(text)
    except json.JSONDecodeError:
        # Find the first balanced [...] using bracket depth tracking
        selections = None
        start = text.find('[')
        if start >= 0:
            depth = 0
            for i in range(start, len(text)):
                if text[i] == '[':
                    depth += 1
                elif text[i] == ']':
                    depth -= 1
                    if depth == 0:
                        try:
                            selections = json.loads(text[start:i+1])
                        except json.JSONDecodeError as e:
                            log.error("Failed to parse Claude JSON: %s\nResponse: %.500s", e, text)
                        break
        if selections is None:
            log.error("No JSON array in Claude response: %.500s", text)
            return []

    # Map back to articles
    curated = []
    for sel in selections:
        idx = sel.get("index")
        if idx is None or not (0 <= idx < len(articles)):
            continue
        a = articles[idx]
        a.score = sel.get("score", 0)
        a.summary = sel.get("summary", "")
        a.tags = sel.get("tags", [])
        a.reason = sel.get("reason", "")
        curated.append(a)

    curated.sort(key=lambda a: a.score, reverse=True)
    log.info("Claude selected %d / %d articles", len(curated), len(articles))
    return curated


# ---------------------------------------------------------------------------
# Notion uploader
# ---------------------------------------------------------------------------


def _notion_request(endpoint: str, token: str, payload: dict, method: str = "POST") -> dict:
    """Make a request to the Notion API."""
    url = f"https://api.notion.com/v1/{endpoint}"
    data = json.dumps(payload).encode("utf-8")
    req = Request(url, data=data, method=method)
    req.add_header("Authorization", f"Bearer {token}")
    req.add_header("Content-Type", "application/json")
    req.add_header("Notion-Version", "2022-06-28")
    try:
        with urlopen(req, timeout=30) as resp:
            return json.loads(resp.read().decode("utf-8"))
    except HTTPError as e:
        body = e.read().decode("utf-8", errors="replace")
        log.error("Notion API error (HTTP %d): %s", e.code, body[:500])
        raise
    except URLError as e:
        log.error("Notion API connection error: %s", e.reason)
        raise


def _estimate_reading_time(description: str) -> str:
    """Estimate reading time based on description length."""
    # description is truncated to 500 chars; estimate full article from ratio
    # Average Korean reading speed: ~500 chars/min, English: ~200 words/min
    char_count = len(description)
    # Rough estimate: description is ~10-20% of full article
    estimated_full = char_count * 7
    minutes = max(1, estimated_full // 500)
    if minutes <= 3:
        return "~3ë¶„"
    elif minutes <= 7:
        return "~5ë¶„"
    elif minutes <= 12:
        return "~10ë¶„"
    else:
        return "10ë¶„+"


def _build_article_blocks(article: Article) -> list[dict]:
    """Build Notion blocks for a single article."""
    tags_text = " Â· ".join(article.tags) if article.tags else ""
    reading_time = _estimate_reading_time(article.description)

    # Callout block with article info
    rich_text = [
        {
            "type": "text",
            "text": {"content": article.title, "link": {"url": article.link}},
            "annotations": {"bold": True},
        },
        {
            "type": "text",
            "text": {"content": f"  {reading_time}  via {article.source}", "link": None},
            "annotations": {"color": "gray", "italic": True},
        },
    ]
    if tags_text:
        rich_text.append(
            {"type": "text", "text": {"content": f"\n{tags_text}", "link": None},
             "annotations": {"color": "gray"}}
        )
    rich_text.append(
        {"type": "text", "text": {"content": f"\n{article.summary}", "link": None}}
    )
    if article.reason:
        rich_text.append(
            {"type": "text", "text": {"content": f"\nðŸ’¬ {article.reason}", "link": None},
             "annotations": {"italic": True, "color": "gray"}}
        )

    return [
        {
            "object": "block",
            "type": "callout",
            "callout": {
                "rich_text": rich_text,
                "icon": {"type": "emoji", "emoji": "\U0001f4cc"},
                "color": "gray_background",
            },
        },
    ]


def _build_notion_blocks(curated: list[Article], errors: list[str]) -> list[dict]:
    """Build all Notion content blocks for the digest page."""
    today = datetime.now().strftime("%Yë…„ %mì›” %dì¼")
    blocks: list[dict] = []

    # Header paragraph
    blocks.append({
        "object": "block",
        "type": "paragraph",
        "paragraph": {
            "rich_text": [{
                "type": "text",
                "text": {"content": f"ì´ {len(curated)}ê±´ì˜ ê´€ë ¨ ê¸°ì‚¬ê°€ ì„ ë³„ë˜ì—ˆìŠµë‹ˆë‹¤ Â· {today}"},
                "annotations": {"color": "gray"},
            }],
        },
    })
    blocks.append({"object": "block", "type": "divider", "divider": {}})

    # No articles message
    if not curated:
        blocks.append({
            "object": "block",
            "type": "callout",
            "callout": {
                "rich_text": [{
                    "type": "text",
                    "text": {"content": "ì˜¤ëŠ˜ì€ ì¶”ì²œ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\ní€„ë¦¬í‹° ë†’ì€ ê¸€ë§Œ ì—„ì„ í•˜ê³  ìžˆìœ¼ë‹ˆ, ì˜¤ëŠ˜ì€ íŽ¸í•˜ê²Œ ì‰¬ì–´ê°€ì„¸ìš”!"},
                }],
                "icon": {"type": "emoji", "emoji": "â˜•"},
                "color": "gray_background",
            },
        })
        blocks.append({"object": "block", "type": "divider", "divider": {}})

    # List articles (already sorted by score descending)
    for a in curated:
        blocks.extend(_build_article_blocks(a))

    if curated:
        blocks.append({"object": "block", "type": "divider", "divider": {}})

    # Errors section
    if errors:
        blocks.append({
            "object": "block",
            "type": "callout",
            "callout": {
                "rich_text": [{
                    "type": "text",
                    "text": {"content": "Fetch Errors:\n" + "\n".join(f"â€¢ {e}" for e in errors)},
                }],
                "icon": {"type": "emoji", "emoji": "\u26a0\ufe0f"},
                "color": "red_background",
            },
        })

    # Footer
    blocks.append({
        "object": "block",
        "type": "paragraph",
        "paragraph": {
            "rich_text": [{
                "type": "text",
                "text": {"content": "Curated by Claude AI Â· news-curator"},
                "annotations": {"color": "gray", "italic": True},
            }],
        },
    })

    return blocks


def upload_to_notion(curated: list[Article], errors: list[str], config: dict):
    """Create a Notion database page with the curated digest."""
    ncfg = config["notion"]
    token = ncfg["token"]
    database_id = ncfg["database_id"]
    today = datetime.now().strftime("%Y-%m-%d")
    title = f"{today} Daily Tech Digest"

    blocks = _build_notion_blocks(curated, errors)

    # Create page with first batch of blocks (max 100 per request)
    page_payload = {
        "parent": {"database_id": database_id},
        "icon": {"type": "emoji", "emoji": "\U0001f4f0"},
        "properties": {
            "ì´ë¦„": {"title": [{"text": {"content": title}}]},
            "ìž‘ì„±ì¼": {"date": {"start": today}},
        },
        "children": blocks[:100],
    }

    log.info("Creating Notion page: %s (%d blocks)", title, len(blocks))
    page = _notion_request("pages", token, page_payload)
    page_id = page["id"]
    page_url = page["url"]

    # Append remaining blocks in batches of 100
    remaining = blocks[100:]
    while remaining:
        batch = remaining[:100]
        remaining = remaining[100:]
        _notion_request(
            f"blocks/{page_id}/children", token,
            {"children": batch}, method="PATCH",
        )
        log.info("Appended %d extra blocks to page", len(batch))

    log.info("Notion page created: %s", page_url)


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    log.info("=== News Curator started ===")
    config = load_config()
    conn = init_db()
    errors: list[str] = []

    # 1. Fetch & parse all feeds
    all_articles: list[Article] = []
    max_per = config["scoring"].get("max_articles_per_source", 15)

    for feed in config["feeds"]:
        name = feed["name"]
        xml = fetch_feed(feed)
        if xml is None:
            errors.append(f"{name}: fetch failed")
            continue

        parsed = parse_feed(xml, name)
        log.info("Parsed %d articles from %s", len(parsed), name)

        new = [a for a in parsed if not is_seen(conn, a.link)]
        log.info("New from %s: %d / %d", name, len(new), len(parsed))
        all_articles.extend(new[:max_per])

    # Filter out articles older than max_age_days
    max_age_days = config["scoring"].get("max_age_days", 3)
    cutoff_dt = datetime.now(timezone.utc) - timedelta(days=max_age_days)
    before_filter = len(all_articles)
    filtered = []
    for a in all_articles:
        dt = parse_pub_date(a.pub_date)
        if dt is None:
            filtered.append(a)  # keep articles with unparseable dates
        elif dt >= cutoff_dt:
            filtered.append(a)
    all_articles = filtered
    if before_filter != len(all_articles):
        log.info("Filtered out %d old articles (> %d days)", before_filter - len(all_articles), max_age_days)

    if not all_articles:
        log.info("No new articles. Done.")
        cleanup_db(conn, config["db"].get("retention_days", 30))
        conn.close()
        return

    log.info("Total new articles for curation: %d", len(all_articles))

    # 2. LLM curation
    curated = curate_with_claude(all_articles, config)

    # Mark ALL fetched articles as seen (avoid re-processing)
    mark_seen(conn, all_articles)

    # 3. Upload to Notion (even if no articles passed curation)
    upload_to_notion(curated, errors, config)

    # 4. Cleanup
    cleanup_db(conn, config["db"].get("retention_days", 30))
    conn.close()
    log.info("=== News Curator finished ===")


def load_config() -> dict:
    with open(CONFIG_PATH, encoding="utf-8") as f:
        return json.load(f)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        log.info("Interrupted by user")
    except Exception as e:
        log.error("Unexpected error: %s", e, exc_info=True)
